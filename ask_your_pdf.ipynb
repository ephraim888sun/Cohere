{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJEGTRzxhdP6"
      },
      "source": [
        "Good embeddings need good contexts to embed. Here's an example of how to combine smart contexts from llmsherpa.LayoutPDFParser with the new Cohere embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQkm8Hgwhym0"
      },
      "source": [
        "First install cohere and it's dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5LhYAk2hryu"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken\n",
        "!pip install openai\n",
        "!pip install -U cohere"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUwrz9cx6xEj"
      },
      "source": [
        "Now instll the llmsherpa library to get LayoutPDFParser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX6yHPP0i2X_",
        "outputId": "3301255e-89c8-435e-d85c-e352743a2c7e"
      },
      "outputs": [],
      "source": [
        "!pip install llmsherpa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "04Aiy1iAi77I"
      },
      "outputs": [],
      "source": [
        "from llmsherpa.readers import LayoutPDFReader\n",
        "\n",
        "llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
        "pdf_url = \"https://www.shinzen.org/wp-content/uploads/2016/08/WhatIsMindfulness_SY_Public_ver1.5.pdf\" # also allowed is a file path e.g. /home/downloads/xyz.pdf\n",
        "pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
        "doc = pdf_reader.read_pdf(pdf_url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98oCyOZB7I9r"
      },
      "source": [
        "Now go through the smart chunks returned from the PDF and embed the chunk texts with Cohere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9-PY7dk-kDLf"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Client.embed() takes 1 positional argument but 2 positional arguments (and 2 keyword-only arguments) were given",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m   contexts\u001b[38;5;241m.\u001b[39mappend(chunk\u001b[38;5;241m.\u001b[39mto_context_text())\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#Encode your documents with input type 'search_document'\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m doc_emb \u001b[38;5;241m=\u001b[39m \u001b[43mco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msearch_document\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membed-english-v3.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39membeddings\n\u001b[1;32m     16\u001b[0m doc_emb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(doc_emb)\n",
            "\u001b[0;31mTypeError\u001b[0m: Client.embed() takes 1 positional argument but 2 positional arguments (and 2 keyword-only arguments) were given"
          ]
        }
      ],
      "source": [
        "import cohere\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from os import environ\n",
        "cohere_key = environ.get('COHERE_API_KEY')\n",
        "\n",
        "co = cohere.Client(cohere_key)\n",
        "\n",
        "contexts = []\n",
        "for chunk in doc.chunks():\n",
        "  contexts.append(chunk.to_context_text())\n",
        "\n",
        "#Encode your documents with input type 'search_document'\n",
        "doc_emb = co.embed(contexts, input_type=\"search_document\", model=\"embed-english-v3.0\").embeddings\n",
        "doc_emb = np.asarray(doc_emb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC0BaTuE7dJI"
      },
      "source": [
        "Now we use cohere question embeddings to embed the queries, search relevant contexts and use OpenAI to summarize the related contexts conditioned on your question!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiQcQ_GXkpwC",
        "outputId": "b65dbb57-1cad-44aa-cfe8-a48a6b736a5c"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from IPython.core.display import display, HTML\n",
        "openai.api_key = \"{use open ai key here}\"\n",
        "\n",
        "def ask(query):\n",
        "  #Encode your query with input type 'search_query'\n",
        "  query_emb = co.embed([query], input_type=\"search_query\", model=\"embed-english-v3.0\").embeddings\n",
        "  query_emb = np.asarray(query_emb)\n",
        "  query_emb.shape\n",
        "\n",
        "  #Compute the dot product between query embedding and document embedding\n",
        "  scores = np.dot(query_emb, doc_emb.T)[0]\n",
        "\n",
        "  #Find the highest scores\n",
        "  max_idx = np.argsort(-scores)\n",
        "  most_relevant_contexts = []\n",
        "  top_k = 10\n",
        "\n",
        "  #Get only the top contexts to keep the context for openai small\n",
        "  for idx in max_idx[0:top_k]:\n",
        "    most_relevant_contexts.append(contexts[idx])\n",
        "\n",
        "  #Call OpenAI to synthesize answers\n",
        "  passages = \"\\n\".join(most_relevant_contexts)\n",
        "  prompt = f\"Read the following passages and answer the question: {query}\\n passages: {passages}\"\n",
        "  completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
        "  synthesized_answer = completion.choices[0].message.content\n",
        "\n",
        "  print(f\"Query: {query}\")\n",
        "  print(f\"Answer: {synthesized_answer}\")\n",
        "  print(\"\\nRelevant contexts: \\n\")\n",
        "  for ctx in most_relevant_contexts:\n",
        "      print(ctx)\n",
        "      print(\"--------\")\n",
        "\n",
        "ask(\"what do i need to practice mindfulness\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
